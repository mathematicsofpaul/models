{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scyE4k8gpwl0"
      },
      "source": [
        "# ENSEMBLE RNN for BIO Classification\n",
        "\n",
        "## Data Prep and Torchtext.legacy\n",
        "\n",
        "We begin by importing the needed python packages. Since the data is in CoNLL format, we can conveniently load the data with torchtext."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "9osUMmompwl2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import copy \n",
        "from tqdm import tqdm\n",
        "from torchtext.legacy import data, datasets\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import random \n",
        "\n",
        "#importing custom helper functions\n",
        "from helpers import *\n",
        "\n",
        "#Seed setting over all possible random components.\n",
        "seed_everything()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "egSkC0DDpwl4"
      },
      "outputs": [],
      "source": [
        "TEXT = data.Field(lower = False) #depending on case sensative or not \n",
        "LABELS = data.Field(unk_token = None)\n",
        "\n",
        "train_data = datasets.SequenceTaggingDataset(\n",
        "                path='./data/NERdata/train.tsv',\n",
        "                fields=[('text', TEXT),\n",
        "                        ('labels', LABELS)])   \n",
        "\n",
        "valid_data = datasets.SequenceTaggingDataset(\n",
        "                path='./data/NERdata/test.tsv',\n",
        "                fields=[('text',  TEXT),\n",
        "                        ('labels', LABELS)])\n",
        "\n",
        "fields = ((\"text\", TEXT), (\"labels\", LABELS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cHlSTlEpwl7"
      },
      "source": [
        "## Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first example is displayed below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCznT-c3pwl8",
        "outputId": "01c721a3-0051-43c0-c14a-d1c06c316fbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': ['Identification', 'of', 'APC2', ',', 'a', 'homologue', 'of', 'the', 'adenomatous', 'polyposis', 'coli', 'tumour', 'suppressor', '.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O']}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIY61Z5Cpwl9"
      },
      "source": [
        "What are the respective sizes of the train and test data? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpadMDOCpwl9",
        "outputId": "704ee46f-a052-4b93-bbb2-e4fe299bd39e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 5424\n",
            "Number of validation examples: 940\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the dictionary and assignment embeddings (Downloading of the embedding will take a bit of time.) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this particular part, we have built a library that associates to each word a numerical value. Then once that is complete, we then assign a word embedding to it with the Glove embedding library. Embedding words gives us a head start when it comes to training our models. By assigning word embeddings we are able to learn at much quicker rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "MGKCnORWpwl_"
      },
      "outputs": [],
      "source": [
        "#builds a vocab library for the training data. \n",
        "TEXT.build_vocab(train_data, \n",
        "                 vectors = \"glove.6B.300d\",\n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "#builds a vocab library for the labels \n",
        "LABELS.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt76JMgPpwmA"
      },
      "source": [
        "These are the sizes for each respective vocab. The additional term in the LABEL dictionary is the padding term. The padding term is important because it will help us later on when we work with sentences with non-uniform length. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0meSOCKpwmB",
        "outputId": "d8440999-07a7-4c38-9f60-ca0167915f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 9286\n",
            "Unique tokens in LABELS vocabulary: 4\n",
            "LABELS vocabulary: ['<pad>', 'O', 'I', 'B']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABELS vocabulary: {len(LABELS.vocab)}\")\n",
        "print(f\"LABELS vocabulary: {LABELS.vocab.itos}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Checking\n",
        "We need to establish a baseline that we wish to overcome. So we look at the percentages of each respective class and calculate the accuracy based on if our model purely guessed. Since the classes are obviously unbalanced we have got to do weighted guessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tag\t\tCount\t\tPercentage\n",
            "\n",
            "O\t\t124452\t\t91.7%\n",
            "I\t\t6115\t\t 4.5%\n",
            "B\t\t5134\t\t 3.8%\n",
            "The baseline to beat is therefore : 84.45%\n"
          ]
        }
      ],
      "source": [
        "print(\"Tag\\t\\tCount\\t\\tPercentage\\n\")\n",
        "percentage = 0 \n",
        "for tag, count, percent in tag_percentage(LABELS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t\\t{count}\\t\\t{percent*100:4.1f}%\") \n",
        "    percentage+=(percent)**2\n",
        "print(f\"The baseline to beat is therefore : {round(percentage,4)*100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct an iterator with BucketIterator\n",
        "\n",
        "Since the sentences vary in all sorts of lengths, padding can get out of control. Hence, we elect to use BucketIterator to load sentences of similar length in order make for a more efficient training process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "78DjTNiJpwmL"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "#assigns the devices for which there will be training on. \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#the iterator to batch cycle through the dataset \n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-KCF_thpwmO"
      },
      "source": [
        "## Defining the GRU module\n",
        "\n",
        "We first start off with a modified GRU module. In particular, we opted for the inclusion of a linear layer post embedding. This is often a reccommended trick to ensure that the model copes with the word embedding well. \n",
        "\n",
        "The reason that we are starting out with GRU is because it much lighter to train than LSTM and Transformer models. By doing this, it allows us to better save on time and cost. This allows us to focus on other things like experimenting with different architectures, training routines and so forth.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "w-Wa3Tp5pwmO"
      },
      "outputs": [],
      "source": [
        "class GRUIOB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim, \n",
        "                 n_layers,\n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.gru = nn.GRU(embedding_dim, \n",
        "                        hidden_dim, \n",
        "                        num_layers = n_layers, \n",
        "                        bidirectional = bidirectional, \n",
        "                        bias = True,\n",
        "                        dropout = dropout if n_layers > 1 else 0)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        #the times 2 addresses the doubling in \n",
        "        \n",
        "        #post embedding fully connected layer\n",
        "        self.emfc = nn.Linear(embedding_dim, embedding_dim)\n",
        "        \n",
        "        #layernorm out of interest \n",
        "        self.layer_norm = nn.LayerNorm(embedding_dim, elementwise_affine=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        # coded included for trials with layer norming\n",
        "        # embedded = self.layer_norm(embedded.permute(1, 0, 2))\n",
        "        # embedded = embedded.permute(1, 0, 2) \n",
        "        # # #layer_normie takes in batch first so we gotta remember to swap it\n",
        "        \n",
        "        post_emb = self.emfc(embedded) \n",
        "        #feed forward connection \n",
        "            \n",
        "        outputs, hidden= self.gru(post_emb)\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hidden dim]\n",
        "    \n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #outputs the final layer output for each word of the sentence \n",
        "        #does NOT include softmax. That is handled by the criterion algorithm\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjQq-Eqpwmi"
      },
      "source": [
        "# Training\n",
        "In terms of the training, in order to counter the problem of exploding gradients, we also included gradient clipping with in the RNN training algorithms. The cut off value is 0.3. \n",
        "\n",
        "\n",
        "## Initialisation - GRU\n",
        "\n",
        "Here we intialise the Bidirectional GRU with 32 embedding dimensions with 3 stacked layers. This design decision was based off several trials that were conducted with varying degrees of hidden dimensions and stacked layers. The bidirectional was chosen because we would like to incorporate information from all points of the sentence. Intuitively, the entity of a particular word is likely influenced by words at all points of the sentence.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "crDf8hTHpwmi"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 32\n",
        "OUTPUT_DIM = len(LABELS.vocab)\n",
        "N_LAYERS = 2 #number of stacked layers 2 to 3 is pretty good \n",
        "BIDIRECTIONAL = True #we elect to use a bidirectional version \n",
        "DROPOUT = 0.25\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "N_EPOCHS = 40\n",
        "\n",
        "model = GRUIOB(INPUT_DIM, \n",
        "                EMBEDDING_DIM, \n",
        "                HIDDEN_DIM, \n",
        "                OUTPUT_DIM, \n",
        "                N_LAYERS, \n",
        "                BIDIRECTIONAL, \n",
        "                DROPOUT, \n",
        "                PAD_IDX)\n",
        "\n",
        "#define model intializer \n",
        "def initialiser(model): \n",
        "    #normal initialisation \n",
        "    def init_weights(m):\n",
        "        for name, param in m.named_parameters():\n",
        "            nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
        "\n",
        "    #applying the normal weights        \n",
        "    model.apply(init_weights)\n",
        "\n",
        "    #embeddding initialisation\n",
        "    pretrained_embeddings = TEXT.vocab.vectors\n",
        "    model.embedding.weight.data.copy_(pretrained_embeddings) \n",
        "\n",
        "    #pad token embedding to zero vector\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "initialiser(model)\n",
        "\n",
        "#optimiser \n",
        "optimizer = optim.Adam(model.parameters()) \n",
        "\n",
        "#ensuring that the pad tokens are ignored during the cross entropy calculation \n",
        "TAG_PAD_IDX = LABELS.vocab.stoi[LABELS.pad_token]\n",
        "\n",
        "#criterion \n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "#sends the models to the GPU \n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUnSxXfmpwmt"
      },
      "source": [
        "## Main Training Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W08Crfvzpwmw",
        "outputId": "b84a3cfe-8f92-423a-a26f-af3b6043800a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRU Model Performance:\n",
            "Total Epochs: 40  | Totals Time: 0m 33s\n",
            "Train Loss: 0.006 | Train Acc: 99.80%\n",
            "Val. Loss: 0.172  |  Val. Acc: 97.42%\n",
            "The model has 2,959,304 trainable parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "#import necessary training functions\n",
        "from helpers import train, evaluate, epoch_time, count_parameters\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "start_time = time.time() \n",
        "loop =  tqdm(range(N_EPOCHS),leave=False)\n",
        "for epoch in loop:\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1.1-model.pt')\n",
        "\n",
        "    loop.set_description(f\"Epoch [{epoch+1}/{N_EPOCHS}]\")\n",
        "    loop.set_postfix(train_loss=train_loss, valid_loss=valid_loss)\n",
        "\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "print(\"GRU Model Performance:\")\n",
        "print(f'Total Epochs: {epoch+1:02}  | Totals Time: {epoch_mins}m {epoch_secs}s')\n",
        "print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "print(f'Val. Loss: {valid_loss:.3f}  |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "#Model assignment for ensemble  \n",
        "model_GRU = copy.deepcopy(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMKuvg9apwmw"
      },
      "source": [
        "We see here is that the performance is quite good with ~97 percent accuracy in the test set. Comparing this with our initial baseline of 84.45%, this indicates that our model is actually meaningful. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN Model Performance\n",
            "Total Epochs: 40  | Totals Time: 0m 33s\n",
            "Train Loss: 0.009| Train Acc: 99.70%\n",
            "Val. Loss: 0.199 |  Val. Acc: 97.24%\n",
            "The model has 2,904,008 trainable parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "class RNNIOB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim, \n",
        "                 n_layers,\n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, \n",
        "                        hidden_dim, \n",
        "                        num_layers = n_layers, \n",
        "                        bidirectional = bidirectional, \n",
        "                        bias = True,\n",
        "                        dropout = dropout if n_layers > 1 else 0, \n",
        "                        nonlinearity = 'relu')\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        #the times 2 addresses the doubling in \n",
        "        \n",
        "        #post embedding fully connected layer\n",
        "        self.emfc = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        post_emb = self.emfc(embedded) \n",
        "        #feed forward connection post embedding\n",
        "            \n",
        "        outputs, hidden= self.rnn(post_emb)\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hidden dim]\n",
        "    \n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #outputs the final layer output for each word of the sentence \n",
        "        #does NOT include softmax. That is handled by the criterion algorithm\n",
        "        return predictions\n",
        "\n",
        "#KEEP THE INITIALISATION FROM BEFORE. \n",
        "\n",
        "model = RNNIOB(INPUT_DIM, \n",
        "                EMBEDDING_DIM, \n",
        "                HIDDEN_DIM, \n",
        "                OUTPUT_DIM, \n",
        "                N_LAYERS, \n",
        "                BIDIRECTIONAL, \n",
        "                DROPOUT, \n",
        "                PAD_IDX)\n",
        "\n",
        "#intialiser\n",
        "initialiser(model)\n",
        "\n",
        "#optimiser \n",
        "optimizer = optim.Adam(model.parameters()) \n",
        "\n",
        "#ensuring that the pad tokens are ignored during the cross entropy calculation \n",
        "TAG_PAD_IDX = LABELS.vocab.stoi[LABELS.pad_token]\n",
        "\n",
        "#criterion \n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "#sends the models to the GPU \n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "#main training lopp \n",
        "best_valid_loss = float('inf')\n",
        "start_time = time.time() \n",
        "loop =  tqdm(range(N_EPOCHS),leave=False)\n",
        "for epoch in loop:\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2.1-model.pt')\n",
        "\n",
        "    loop.set_description(f\"Epoch [{epoch+1}/{N_EPOCHS}]\")\n",
        "    loop.set_postfix(train_loss=train_loss, valid_loss=valid_loss)\n",
        "    \n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "print(\"RNN Model Performance\")\n",
        "print(f'Total Epochs: {epoch+1:02}  | Totals Time: {epoch_mins}m {epoch_secs}s')\n",
        "print(f'Train Loss: {train_loss:.3f}| Train Acc: {train_acc*100:.2f}%')\n",
        "print(f'Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters') \n",
        "\n",
        "#Model assignment for ensemble  \n",
        "model_RNN = copy.deepcopy(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simply with the inclusion of relu activations in the RNN, we are getting comparable results to GRUs. ~97 accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM \n",
        "Now we try a bidirectional LSTM with 3 stacked layers with hidden state embeddings that have dimension 32. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM Model Performance\n",
            "Total Epochs: 40  | Totals Time: 0m 32s\n",
            "Train Loss: 0.004| Train Acc: 99.88%\n",
            "Val. Loss: 0.205 |  Val. Acc: 97.14%\n",
            "The model has 2,896,652 trainable parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "class LSTMIOB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim, \n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            num_layers = n_layers, \n",
        "                            bidirectional = bidirectional,\n",
        "                            dropout = dropout if n_layers > 1 else 0)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        #the times 2 addresses the doubling in \n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hidden dim]\n",
        "        \n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "model = LSTMIOB(INPUT_DIM, \n",
        "                EMBEDDING_DIM, \n",
        "                HIDDEN_DIM, \n",
        "                OUTPUT_DIM, \n",
        "                N_LAYERS, \n",
        "                BIDIRECTIONAL, \n",
        "                DROPOUT, \n",
        "                PAD_IDX)\n",
        "\n",
        "##intialiser\n",
        "initialiser(model)\n",
        "\n",
        "#optimiser \n",
        "optimizer = optim.Adam(model.parameters()) \n",
        "\n",
        "#ensuring that the pad tokens are ignored during the cross entropy calculation \n",
        "TAG_PAD_IDX = LABELS.vocab.stoi[LABELS.pad_token]\n",
        "\n",
        "#criterion \n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "#sends the models to the GPU \n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "#main training loop\n",
        "best_valid_loss = float('inf')\n",
        "start_time = time.time() \n",
        "loop =  tqdm(range(N_EPOCHS),leave=False)\n",
        "for epoch in loop:\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut3.1-model.pt')\n",
        "\n",
        "    loop.set_description(f\"Epoch [{epoch+1}/{N_EPOCHS}]\")\n",
        "    loop.set_postfix(train_loss=train_loss, valid_loss=valid_loss)\n",
        "\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "print(\"LSTM Model Performance\")\n",
        "print(f'Total Epochs: {epoch+1:02}  | Totals Time: {epoch_mins}m {epoch_secs}s')\n",
        "print(f'Train Loss: {train_loss:.3f}| Train Acc: {train_acc*100:.2f}%')\n",
        "print(f'Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "#Model assignment for ensemble \n",
        "model_LSTM = copy.deepcopy(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same story is told with the lstm model compared to the GRU. They are modelled very similarly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since these models are so great, we can look to combine them together to form an even more powerful model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Epochs: 40  | Totals Time: 0m 40s\n",
            "Train Loss: 0.005| Train Acc: 99.89%\n",
            "Val. Loss: 0.175 |  Val. Acc: 97.31%\n",
            "The model has 52 trainable parameters (linear layer trained only)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "class MyEnsemble(nn.Module):\n",
        "    def __init__(self, modelA, modelB, modelC, output_dim):\n",
        "        super(MyEnsemble, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "        self.modelC = modelC\n",
        "        self.output = nn.Linear(output_dim*3, output_dim)\n",
        "    def forward(self, text):\n",
        "        x1 = self.modelA(text)\n",
        "        x2 = self.modelB(text)\n",
        "        x3 = self.modelC(text)\n",
        "        x = torch.cat((x1, x2, x3), dim=2)  \n",
        "        final = self.output(x)\n",
        "        return final\n",
        "\n",
        "#freeze models \n",
        "for param in model_GRU.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model_LSTM.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model_RNN.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#intialise model \n",
        "model = MyEnsemble(model_GRU,model_RNN, model_LSTM ,OUTPUT_DIM)\n",
        "\n",
        "#optimiser \n",
        "optimizer = optim.Adam(model.parameters()) \n",
        "\n",
        "#ensuring that the pad tokens are ignored during the cross entropy calculation \n",
        "TAG_PAD_IDX = LABELS.vocab.stoi[LABELS.pad_token]\n",
        "\n",
        "#criterion \n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "#sends the models to the GPU \n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "N_EPOCHS = 40\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "start_time = time.time() \n",
        "loop =  tqdm(range(N_EPOCHS),leave=False)\n",
        "for epoch in loop:\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut4.1-model.pt')\n",
        "\n",
        "    loop.set_description(f\"Epoch [{epoch+1}/{N_EPOCHS}]\")\n",
        "    loop.set_postfix(train_loss=train_loss, valid_loss=valid_loss)\n",
        "\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "print(f'Total Epochs: {epoch+1:02}  | Totals Time: {epoch_mins}m {epoch_secs}s')\n",
        "print(f'Train Loss: {train_loss:.3f}| Train Acc: {train_acc*100:.2f}%')\n",
        "print(f'Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters (linear layer trained only)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the ensemble comes out as one of the better models. However, it could be debated that for simplicity sake, it would have been better to train either the LSTM or GRUs strictly instead. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zAXE1KTpwmy"
      },
      "source": [
        "# Sample checking for the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hixzgb6fpwmz",
        "outputId": "2e9849f7-009d-41b7-8951-0c931e1522b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pred. Tag\tActual Tag\tCorrect?\tToken\n",
            "\n",
            "O\t\tO\t\t✔\t\tThe\n",
            "B\t\tB\t\t✔\t\tadenomatous\n",
            "I\t\tI\t\t✔\t\tpolyposis\n",
            "I\t\tI\t\t✔\t\tcoli\n",
            "I\t\tI\t\t✔\t\t(\n",
            "I\t\tI\t\t✔\t\tAPC\n",
            "I\t\tI\t\t✔\t\t)\n",
            "I\t\tI\t\t✔\t\ttumour\n",
            "O\t\tO\t\t✔\t\t-\n",
            "O\t\tO\t\t✔\t\tsuppressor\n",
            "O\t\tO\t\t✔\t\tprotein\n",
            "O\t\tO\t\t✔\t\tcontrols\n",
            "O\t\tO\t\t✔\t\tthe\n",
            "O\t\tO\t\t✔\t\tWnt\n",
            "O\t\tO\t\t✔\t\tsignalling\n",
            "O\t\tO\t\t✔\t\tpathway\n",
            "O\t\tO\t\t✔\t\tby\n",
            "O\t\tO\t\t✔\t\tforming\n",
            "O\t\tO\t\t✔\t\ta\n",
            "O\t\tO\t\t✔\t\tcomplex\n",
            "O\t\tO\t\t✔\t\twith\n",
            "O\t\tO\t\t✔\t\tglycogen\n",
            "O\t\tO\t\t✔\t\tsynthase\n",
            "O\t\tO\t\t✔\t\tkinase\n",
            "O\t\tO\t\t✔\t\t3beta\n",
            "O\t\tO\t\t✔\t\t(\n",
            "O\t\tO\t\t✔\t\tGSK\n",
            "O\t\tO\t\t✔\t\t-\n",
            "O\t\tO\t\t✔\t\t3beta\n",
            "O\t\tO\t\t✔\t\t)\n",
            "O\t\tO\t\t✔\t\t,\n",
            "O\t\tO\t\t✔\t\taxin\n",
            "O\t\tO\t\t✔\t\t/\n",
            "O\t\tO\t\t✔\t\tconductin\n",
            "O\t\tO\t\t✔\t\tand\n",
            "O\t\tO\t\t✔\t\tbetacatenin\n",
            "O\t\tO\t\t✔\t\t.\n"
          ]
        }
      ],
      "source": [
        "example_index = 1\n",
        "sentence = vars(train_data.examples[example_index])['text']\n",
        "actual_tags = vars(train_data.examples[example_index])['labels'] \n",
        "tokens, pred_tags, unks = tag_sentence(model, \n",
        "                                       device, \n",
        "                                       sentence, \n",
        "                                       TEXT, \n",
        "                                       LABELS)\n",
        "\n",
        "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n",
        "\n",
        "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
        "    correct = '✔' if pred_tag == actual_tag else '✘'\n",
        "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2o_BtQspwm2"
      },
      "source": [
        "The results are great considering the perfect matches across the table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sample checking for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pred. Tag\tActual Tag\tCorrect?\tToken\n",
            "\n",
            "B\t\tB\t\t✔\t\tAtaxia\n",
            "I\t\tI\t\t✔\t\t-\n",
            "I\t\tI\t\t✔\t\ttelangiectasia\n",
            "O\t\tO\t\t✔\t\t(\n",
            "B\t\tB\t\t✔\t\tA\n",
            "I\t\tI\t\t✔\t\t-\n",
            "I\t\tI\t\t✔\t\tT\n",
            "O\t\tO\t\t✔\t\t)\n",
            "O\t\tO\t\t✔\t\tis\n",
            "O\t\tO\t\t✔\t\ta\n",
            "B\t\tB\t\t✔\t\trecessive\n",
            "I\t\tI\t\t✔\t\tmulti\n",
            "I\t\tI\t\t✔\t\t-\n",
            "I\t\tI\t\t✔\t\tsystem\n",
            "I\t\tI\t\t✔\t\tdisorder\n",
            "O\t\tO\t\t✔\t\tcaused\n",
            "O\t\tO\t\t✔\t\tby\n",
            "O\t\tO\t\t✔\t\tmutations\n",
            "O\t\tO\t\t✔\t\tin\n",
            "O\t\tO\t\t✔\t\tthe\n",
            "O\t\tO\t\t✔\t\tATM\n",
            "O\t\tO\t\t✔\t\tgene\n",
            "O\t\tO\t\t✔\t\tat\n",
            "O\t\tO\t\t✔\t\t11q22\n",
            "O\t\tO\t\t✔\t\t-\n",
            "O\t\tO\t\t✔\t\tq23\n",
            "O\t\tO\t\t✔\t\t(\n",
            "O\t\tO\t\t✔\t\tref\n",
            "O\t\tO\t\t✔\t\t.\n",
            "O\t\tO\t\t✔\t\t3\n",
            "O\t\tO\t\t✔\t\t)\n",
            "O\t\tO\t\t✔\t\t.\n"
          ]
        }
      ],
      "source": [
        "example_index = 1\n",
        "sentence = vars(valid_data.examples[example_index])['text']\n",
        "actual_tags = vars(valid_data.examples[example_index])['labels'] \n",
        "tokens, pred_tags, unks = tag_sentence(model, \n",
        "                                       device, \n",
        "                                       sentence, \n",
        "                                       TEXT, \n",
        "                                       LABELS)\n",
        "\n",
        "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n",
        "\n",
        "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
        "    correct = '✔' if pred_tag == actual_tag else '✘'\n",
        "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "1 - BiLSTM for PoS Tagging.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
